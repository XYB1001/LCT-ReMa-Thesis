Logisitc Regression (default params)

TRAIN: Everything, TEST: Everything

word + char grams (3,6)
splitting (800)

**************************************************
Results for Logisctic Regerssion model:
--------------------------------------------------
Accuracy: 0.6728422089650266
--------------------------------------------------
Precision, recall and F-score per class:
            Precision     Recall    F-score
a1           0.965651   0.859781   0.909646
a2           0.914420   0.738303   0.816978
b1           0.747528   0.560863   0.640880
b2           0.655572   0.508110   0.572498
c1           0.641772   0.797052   0.711033
c2           0.620876   0.563607   0.590857
--------------------------------------------------
Average (macro) F-score: 0.7069820037964466
--------------------------------------------------
Confusion matrix:
Labels: ['a1', 'a2', 'b1', 'b2', 'c1', 'c2']
[[ 1490    20    20    22   130    51]
 [   24  1357    73    57   233    94]
 [   12    40  2419   203  1271   368]
 [    7    28   174  2600  1709   599]
 [    6    32   350   715 11790  1899]
 [    4     7   200   369  3238  4931]]

**************************************************

6 classes
2409141 features
------------------------------
Top 10 predictive features by class:
a1:     char__ hoe word__hi char__hoe word__fr char__ hi char__e e char__ e♂️ char__e♂️ char__ e♂ char__ mf
a2:     word__feas char__yo 1 char__feas word__sonic char__o 1 char__sonic char__t do t word__bac char__soni char__24k
b1:     char__ ?? word__ugh word__wig char__wig char__ wig char__kld char__ !! char__oui char__ !!! char__nct
b2:     char__ —  char__ ^^ char__*-* char__ *- char__ *-* char__ x" char__ .. char__ |  char__. e e word__bea
c1:     char__||  char__¦¦  char__...  char__//  char__.... char__ :' char__bts word__hux char__mfc char__ hux
c2:     char__ ❤  char__... char__! e char__esc char__❤ e char__ e❤ char__ ❤ e char__rt)  char__?!  word__haha


TRAIN: Efcamdat TEST: SM (5000)

**************************************************
Results for LogReg model:
--------------------------------------------------
Accuracy: 0.1206
--------------------------------------------------
Precision, recall and F-score per class:
            Precision     Recall    F-score
a1           0.010040   0.500000   0.019685
a2           0.013255   0.176471   0.024658
b1           0.091248   0.101240   0.095984
b2           0.124444   0.168675   0.143223
c1           0.476553   0.160615   0.240256
c2           0.359223   0.026056   0.048588
--------------------------------------------------
Average (macro) F-score: 0.0953988854491658
--------------------------------------------------
Confusion matrix:
Labels: ['a1', 'a2', 'b1', 'b2', 'c1', 'c2']
[[ 20   4   2   6   7   1]
 [ 26   9   4   3   8   1]
 [233  67  49  71  59   5]
 [284  97  83 112  77  11]
 [942 300 246 429 376  48]
 [487 202 153 279 262  37]]

**************************************************
6 classes
1285177 features
------------------------------
Top 10 predictive features by class:
a1:	word__hi char__'m  char__ hi word__buy char__i'm char__eat char__. be char__ sh char__liv char__i'm 
a2:	char__est char__s bo char__dog char__ dog char__ mo char__isa char__m a char__ wear char__ve t char__wear
b1:	char__tim word__tim char__r.  char__ blog char__blog word__blog char__song char__ tv char__it  char__y o
b2:	word__law char__ law  char__law  char__ende char__ham char__law char__ pay char__tic char__ary i char__liti
c1:	char__paula char__aula char__ paula word__paula char__rime char__ paul char__paul char__ cri char__ pau char__pau
c2:	char__obo char__obot char__robot char__robo char__ robot char__ robo char__rit char__ rob char__tres char__ stres



TRAIN: Efacamdat + Twitter, TEST: Reddit

word + char grams (3,6)
splitting (800)

**************************************************
Results for LogReg model:
--------------------------------------------------
Accuracy: 0.3647671391379809
--------------------------------------------------
Precision, recall and F-score per class:
            Precision     Recall    F-score
a1           0.000000   0.000000   0.000000
a2           0.000000   0.000000   0.000000
b1           0.152905   0.078247   0.103520
b2           0.258824   0.084323   0.127204
c1           0.440614   0.568722   0.496538
c2           0.282549   0.375858   0.322592
--------------------------------------------------
Average (macro) F-score: 0.17497572453349444
--------------------------------------------------
Confusion matrix:
Labels: ['a1', 'a2', 'b1', 'b2', 'c1', 'c2']
[[   0    0    1    0    9   10]
 [   0    0    2   21   44   19]
 [   6    1   50   64  324  194]
 [   8    6   81  220 1605  689]
 [   5    6  118  377 2582 1452]
 [   1    6   75  168 1296  931]]

**************************************************
6 classes
2564419 features
------------------------------
Top 10 predictive features by class:
a1:     word__fr char__ mf char__ hoe word__emo word__hi char__ hi char__hoe char__i️  char__ fr char__ asf
a2:     word__sonic char__o 1 char__of* char__sonic char__yo 1 word__feas char__feas char__soni word__real word__bac
b1:     char__ ?? word__ugh word__wig char__wig char__ wig word__ten char__ !! char__kld char__oui char__ js
b2:     char__ ^^ char__ —  char__*-* char__ *- char__ *-* char__ |  word__xd char__ xd char__ .. char__ugu
c1:     char__||  char__¦¦  char__//  char__...  char__.... char__ :' char__bts word__hux char__iked a char__mfc
c2:     char__ ❤  char__... char__esc char__rt)  word__rt char__rt) char__t)  char__(rt char__(rt)  char__(rt)



TRAIN: Efacamdat + Reddit, TEST: Twitter

word + char grams (3,6)
splitting (800)

**************************************************
Results for LogReg model:
--------------------------------------------------
Accuracy: 0.36324663394174467
--------------------------------------------------
Precision, recall and F-score per class:
            Precision     Recall    F-score
a1           0.012285   0.005507   0.007605
a2           0.007117   0.001653   0.002683
b1           0.112934   0.016967   0.029501
b2           0.118979   0.223509   0.155292
c1           0.464874   0.627410   0.534049
c2           0.314421   0.134904   0.188802
--------------------------------------------------
Average (macro) F-score: 0.15298859570254128
--------------------------------------------------
Confusion matrix:
Labels: ['a1', 'a2', 'b1', 'b2', 'c1', 'c2']
[[    5     1     1   233   613    55]
 [    7     2     7   268   766   160]
 [   54    26   179  2219  6798  1274]
 [   67    42   189  2759  7805  1482]
 [  173   124   669 11557 31895  6418]
 [  101    86   540  6153 20733  4306]]

**************************************************
6 classes
1642220 features
------------------------------
Top 10 predictive features by class:
a1:     word__hi char__ hi char__.i  char__ i  char__'m  char__ sh char__e.  char__ .  char__go  char__y.
a2:     char__.i  char__e.  char__ i  char__n.  char__s bo char__t.  char__.th char__ mo char__ien  char__ent
b1:     word__tim char__gar char__yes,  char__yes, char__i w char__r.  word__hi char__s.. char__e not  char__ tim
b2:     char__’s  char__n’t char__n’t  char__’t  char__e i  char__yust  char__yust word__yust char__yus word__yes
c1:     char__lol char__ &  char__y to g word__lol char__l i  char__tai char__sch char__w s char__it! word__oh
c2:     char__ ps char__run char__ted  char__d then char__amn char__, and  char__e to t char__tres word__sb char__cel


